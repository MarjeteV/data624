---
title: "Data 624 Neural Networks"
author: "Kim Koon"
date: "`r Sys.Date()`"
output:
  html_document
editor_options: 
  chunk_output_type: console
---

### Load packages  

```{r load-packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(imputeTS)
library(corrplot)
library(correlation)
library(caret)
library(randomForest)
library(doParallel)
library(xgboost)
```

```{r setup, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

# Neural Networks

```{r message=FALSE, warning=FALSE}
StudentData <- read.csv("https://raw.githubusercontent.com/MarjeteV/data624/refs/heads/main/imputed_test_data.csv")

# Parallel processing
cores <- detectCores()
cl <- makeCluster(cores)
registerDoParallel(cl)
```


```{r message=FALSE, warning=FALSE}
set.seed(8675309)
trainIndex <- createDataPartition(StudentData$PH, p = 0.75, list = FALSE)
trainData <- StudentData[trainIndex, ]
testData <- StudentData[-trainIndex, ]
# Separate x, y, train
x_train <- trainData[, names(trainData) != "PH"]
y_train <- trainData$PH
x_test <- testData[, names(testData) != "PH"]
y_test <- testData$PH

# tuning grid
nnetGrid <- expand.grid(
 size = 1:20,
 decay = c(0.01,0.05, 0.1, 0.5, 1, 2),
 bag = c(TRUE,FALSE))

# model
nnetTuned <- train(
 x = x_train,
 y = y_train,
 method = "avNNet", 
 tuneGrid = nnetGrid,
 preProc = c("center", "scale"),
 trControl = trainControl(method = "cv"),
 linout = TRUE,
 trace = FALSE,
 maxit = 500,
 MaxNWts = 20 * (ncol(x_train) + 1) + 20 + 1)
# Model results
nnetTuned$results
nnetTuned$bestTune
# Tuning plot
plot(nnetTuned)
# Test set predictions & metrics
nnetPred <- predict(nnetTuned, newdata = x_test)
postResample(pred = nnetPred, obs = y_test)
# Variable importance
varImp(nnetTuned)
plot(varImp(nnetTuned))
```

```{r message=FALSE, warning=FALSE}
set.seed(8675309)
# Split data into training/test sets
trainIndex <- createDataPartition(StudentData$PH, p = 0.75, list = FALSE)
trainData <- StudentData[trainIndex, ]
testData <- StudentData[-trainIndex, ]
# Separate x, y, train
x_train <- trainData[, names(trainData) != "PH"]
y_train <- trainData$PH
x_test <- testData[, names(testData) != "PH"]
y_test <- testData$PH

# tuning grid
nnetGrid2 <- expand.grid(
 size = 1:20,
 decay = c(0.01,0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09),
 bag = FALSE)

# model
nnetTuned2 <- train(
 x = x_train,
 y = y_train,
 method = "avNNet", 
 tuneGrid = nnetGrid2,
 preProc = c("center", "scale"),
 trControl = trainControl(method = "cv"),
 linout = TRUE,
 trace = FALSE,
 maxit = 1000,
 MaxNWts = 20 * (ncol(x_train) + 1) + 20 + 1)
# Model results
nnetTuned2$results
nnetTuned2$bestTune
# Tuning plot
plot(nnetTuned2)
# Test set predictions & metrics
nnetPred2 <- predict(nnetTuned2, newdata = x_test)
postResample(pred = nnetPred2, obs = y_test)
# Variable importance
varImp(nnetTuned2)
plot(varImp(nnetTuned2))
```


```{r}
# Prepare data for XGBoost
xgb_train <- xgb.DMatrix(data = as.matrix(x_train),label = trainData$PH)
xgb_test <- xgb.DMatrix(data = as.matrix(x_test),label = testData$PH)


```

```{r}
# Fit XGBoost model
xgboostGrid <- expand.grid(
  nrounds = c(100, 200, 300),       # Number of boosting rounds
  max_depth = c(3, 5, 7),           # Maximum tree depth
  eta = c(0.01, 0.1, 0.2),          # Learning rate
  gamma = c(0, 1, 5),               # Minimum loss reduction for splits
  colsample_bytree = c(0.6, 0.8, 1), # Fraction of columns sampled per tree
  min_child_weight = c(1, 3, 5),    # Minimum sum of weights in a child node
  subsample = c(0.7, 0.8, 1)        # Fraction of data sampled per boosting round
)
# Train xgboost 
xgboostTuned <- train(
 x = as.matrix(x_train),
 y = trainData$PH,
 method = "xgbTree", 
 tuneGrid = xgboostGrid,
 preProc = c("center", "scale"),
 trControl = trainControl(method = "cv")
)
xgboostTuned$bestTune
xgb_tuned_pred <- predict(xgboostTuned, as.matrix(x_test))
postResample(pred = xgb_tuned_pred, obs = testData$PH)

```
```{r}
# Fit XGBoost model
xgboostGrid2 <- expand.grid(
  nrounds = c(200, 300, 400),
  max_depth = c(3, 5, 7, 9, 11),
  eta = c(0.01, 0.1, 0.2),
  gamma = c(0, 1, 5),
  colsample_bytree = c(0.6, 0.8, 1),
  min_child_weight = c(1, 3, 5),
  subsample = c(0.7, 0.8, 1)
)
# Train xgboost 
xgboostTuned2 <- train(
 x = as.matrix(x_train),
 y = trainData$PH,
 method = "xgbTree", 
 tuneGrid = xgboostGrid2,
 preProc = c("center", "scale"),
 trControl = trainControl(method = "cv")
)
xgboostTuned2$bestTune
xgb_tuned_pred2 <- predict(xgboostTuned2, as.matrix(x_test))
postResample(pred = xgb_tuned_pred2, obs = testData$PH)

```

```{r}

xgb_predictions <- predict(xgb_model, xgb_test)
xgb_predictions
```
