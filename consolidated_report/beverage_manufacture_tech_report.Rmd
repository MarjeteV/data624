---
title: "Final Project"
author: "Matthew Tillmawitz, Heleine Fouda, Marjete Vucinaj, Lewris Mota, Kim Koon "
date: "2024-12-15"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    always_allow_html: true
  word_document:
    toc: true
  pdf_document:
    toc: true
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(gridExtra)
library(ggplot2)
library(cowplot)
options(scipen=10000)
library(tidyverse)
library(caret)
library(kableExtra)
library(mice)
library(fpp3)
library(readxl)
library(doParallel)
library(naniar)
library(VIM)
library(magrittr)
library(pls)

set.seed(8675309)

```

```{r include=FALSE}
# Register parallel backend
cl <- makeCluster(detectCores() - 1)

registerDoParallel(cl)
```

### 1. Introduction

Recent regulatory changes emphasize the importance of having a comprehensive understanding of manufacturing processes and their impact on product quality. At ABC Beverage, pH levels are a critical parameter for ensuring consistency and maintaining product standards. This analysis aims to identify and quantify the factors driving pH variability while developing a predictive model that meets these new regulatory standards. By leveraging advanced analytical methods and predictive modeling techniques, the analysis provides data-driven insights to ensure compliance and enhance understanding of the production process. The scope of this analysis includes data preprocessing, the identification of predictive factors, and the development and validation of a robust predictive model for pH levels. The focus is on exploring relationships within the manufacturing data to uncover insights that can drive better decision-making. By implementing this structured approach, the findings will provide actionable insights into the key drivers of pH variability and a reliable framework for prediction.

Understanding and predicting pH levels is crucial not only for meeting regulatory standards but also for maintaining operational excellence. Accurate prediction will enable more efficient quality control, reducing variability and ensuring that ABC Beverage consistently delivers high-quality products. The results of this analysis will empower the company to address compliance needs while optimizing its manufacturing process, underscoring the importance of data-driven decision-making in the production environment.

### 2. Dataset Overview

The dataset comprises observations collected from a beverage production line, capturing information about carbonation levels, filling processes, environmental conditions, and quality control metrics. Each row represents a single production instance, and each column corresponds to a specific variable measured or controlled during the process.

```{r}
# Create a dataframe in R
variables <- data.frame(
  Feature = c(
    "Brand Code", "Carb Volume", "Fill Ounces", "PC Volume", "Carb Pressure", 
    "Carb Temp", "PSC", "PSC Fill", "PSC CO2", "Mnf Flow", "Carb Pressure1", 
    "Fill Pressure", "Hyd Pressure1", "Hyd Pressure2", "Hyd Pressure3", "Hyd Pressure4",
    "Filler Level", "Filler Speed", "Temperature", "Usage cont", "Carb Flow", 
    "Density", "MFR", "Balling", "Pressure Vacuum", "PH", "Oxygen Filler", 
    "Bowl Setpoint", "Pressure Setpoint", "Air Pressure", "Alch Rel", "Carb Rel", 
    "Balling Lvl"
  ),
  Description = c(
    "Unique identifier for the product's brand.",
    "Volume of carbon dioxide dissolved in the product.",
    "Volume of liquid dispensed into each container.",
    "Process control volume for monitoring liquid levels.",
    "Pressure level during carbonation.",
    "Temperature during carbonation for CO2 solubility.",
    "Process Setpoint Control for maintaining parameter targets.",
    "Filling setpoint under controlled conditions.",
    "Setpoint for CO2 levels during carbonation.",
    "Manufacturing flow rate for liquid or gas.",
    "Secondary carbonation pressure reading.",
    "Pressure applied during filling operations.",
    "Hydraulic pressure reading 1 for machine operation.",
    "Hydraulic pressure reading 2 for machine operation.",
    "Hydraulic pressure reading 3 for machine operation.",
    "Hydraulic pressure reading 4 for machine operation.",
    "Measurement of product level in containers.",
    "Speed of the filling machine or process.",
    "Temperature of the process environment.",
    "Container usage or consumption metrics.",
    "Flow rate of CO2 during carbonation.",
    "Density of the product for consistency monitoring.",
    "Mass flow rate of the material through the system.",
    "Sugar concentration level measured by the Balling scale.",
    "Vacuum pressure in the system.",
    "Acidity level of the product.",
    "Oxygen levels in the filling process.",
    "Target setpoint for intermediate container levels.",
    "Desired pressure level in the process.",
    "Air pressure measurement in the system.",
    "Alcohol release or related parameter.",
    "Carbonation release or related measurement.",
    "Sugar concentration level in the final product."
  ),
  Type = c(
    "Categorical", "Numerical", "Numerical", "Numerical", "Numerical",
    "Numerical", "Numerical", "Numerical", "Numerical", "Numerical", 
    "Numerical", "Numerical", "Numerical", "Numerical", "Numerical", 
    "Numerical", "Numerical", "Numerical", "Numerical", "Numerical",
    "Numerical", "Numerical", "Numerical", "Numerical", "Numerical", 
    "Numerical", "Numerical", "Numerical", "Numerical", "Numerical", 
    "Numerical", "Numerical", "Numerical"
  ),
  stringsAsFactors = FALSE
)

variables |> kable(caption = "Beverage Manufacture Process Features") |> kable_styling() |>  kable_classic()
```

The variables play a crucial role in capturing and monitoring various aspects of the production process, directly impacting product quality and operational efficiency:

-   **Carbonation Variables**: These variables (e.g., `Carb Volume`, `Carb Pressure`, and `Carb Temp`) ensure the beverage achieves the desired level of fizziness and retains CO2 effectively. They are critical for meeting product specifications and customer satisfaction.

-   **Filling Variables**:\
    Variables like `Fill Ounces`, `PC Volume`, and `Filler Speed` ensure accurate and consistent product volume in containers, minimizing waste and maintaining packaging standards.

-   **Quality Control Metrics**: Metrics such as `Density`, `Balling`, and `PSC` monitor the chemical and physical properties of the beverage, including sugar concentration, density, and carbonation, which significantly influence the product's pH balance. Maintaining the appropriate pH ensures flavor stability, microbial safety, and overall product quality.

-   **Process Control Variables**: Variables like `PSC CO2` and `PSC` act as setpoints to maintain optimal operational conditions, reducing variability and enhancing production reliability.

### 3. Exploratory Data Analysis

### 4. Data Preparation

```{r, include=FALSE}
test <- read.csv("https://raw.githubusercontent.com/MarjeteV/data624/refs/heads/main/test%20data.csv")
tidy_train <- read.csv("https://raw.githubusercontent.com/MarjeteV/data624/refs/heads/main/training%20data.csv") |>
  as_tibble()
```

Before deciding how to handle missing values in our data we need to gain a better understanding of what data is missing. Looking at the columns with the most missing data in the figure below we can see there is an outlier in MFR, with roughly four times as many missing values as the next most missing variable. The majority of the remaining variables are missing in less than 2% of the observations, so we can be comfortable imputing values will not skew the results. It should be noted there are several observations with missing records for PH. As PH is the value we are attempting to predict, and the rows with missing PH values make up such a small proportion of the data we will be dropping these rows from our training set. The MFR column does not have enough missing values to justify excluding it from our modeling, so we will include it in our imputation. Before beginning to impute the missing values, we need to explore if there are patterns to the missingness.

```{r percent missing, message=FALSE, warning=FALSE}
miss_var_summary(tidy_train) |>
  rename(`Missing Count` = n_miss, `Percent Missing` = pct_miss) |>
  kable() |>
  kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F)
```

Before moving on, it should be noted that Brand.Code does have values which could be considered “missing” as seen in the table below. If we compare the number of observations with no brand code to the missing predictors above we can see it is actually the second most missing column. As Brand.Code can be converted to a factor and one hot encoded, we can treat the “missing” codes as another valid level. Doing so will allow some of our models to use any predictive power present in the missing values without hampering models that are unable to make use of missing values. We will therefore not be imputing values for the missing Brand.Codes.

```{r demonstrate brand values, message=FALSE, warning=FALSE}
tidy_train |>
  count(Brand.Code) |>
  arrange(desc(n))|> 
  kable() |>
  kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F)
```

As a last check before imputing values, we need to examine whether there are any strong patterns in the missingness of our data. Looking at the frequency plot below we can see that while there is not a significant pattern to the missingness of most variables, Filler.Speed does appear to be missing with MFR frequently. This is not necessarily surprising however, as MFR is our most missing variable by far. It could be argued that the relatively high level of correlated missingness between the two variables justifies using an imputation technique that accounts for the correlated missingness such as Multivariate Imputations by Chained Equations (MICE). Referring back to our missing counts however, we can see that Filler.Speed only has 57 missing observations. Our correlation analysis indicates that less than half of these missing observations are also missing MFR. Given 30 of our predictors have any missing data, having one pair of predictors exhibiting an apparently significant correlation is still consistent with random missingness. As no other predictors exhibit a high level of correlated missingness we can conclude there is no relationship between missing values in predictors and, even if there were, using a method such as MICE would not provide meaningful value for such a small number of potentially correlated missing values.

```{r check for patterns in missing data, message=FALSE, warning=FALSE}
aggr_plot <- aggr(tidy_train, 
                  col=c('navyblue','red'), 
                  numbers=TRUE, 
                  sortVars=TRUE, 
                  labels=names(tidy_train), 
                  cex.axis=.7, 
                  gap=3, 
                  ylab=c("Proportion Missing","Pattern"), 
                  only.miss=TRUE, 
                  sortCombs=TRUE,
                  combined=TRUE)
```

Having determined what we will do with the special case variables of PH and Brand.Code, as well as determining there is not a significant level of correlated missingness in the remaining predictors, we can safely impute the remaining missing values. We will be using bootstrap aggregation (bagging) as our method of choice for imputing the missing values. We chose this method as it is able to make use of other predictors with missing values when imputing. Given the large proportion of predictors with any missing values this is a necessary trait and will result in better predictions.

```{r impute training values, echo=TRUE, warning=FALSE}
# Turn integer columns into numerics to prevent errors
tidy_train %<>% mutate(across(
  .cols = c('Filler.Speed', 'Hyd.Pressure4', 'Bowl.Setpoint', 'Carb.Flow'),
  .fns = as.numeric
  ))

# Dropping rows with missing Brand.Code or PH
imputation_set <- tidy_train |>
  filter(!is.na(PH))

# Create a one hot encoding tibble
dummies <- dummyVars( ~ Brand.Code, data = imputation_set)
one_hot_df <- predict(dummies, newdata = imputation_set) |>
  as_tibble()

# Add the one hot df to the original and remove the categorical column
one_hot_df <- imputation_set |>
  cbind(one_hot_df) |>
  select(-Brand.Code)

preprocessor <- preProcess(one_hot_df,
                           method = "bagImpute",
                           allowParallelUpdates = TRUE) 

imputed_data <- predict(preprocessor, newdata = one_hot_df)
```

```{r write imputed values}
imputed_data |>
  write_csv("imputed_test_data.csv")
```

### 5. Model Development {.tabset}

This section outlines the methodology for building and implementing the predictive model. It includes details on data preprocessing, feature selection, hyperparameter tuning, and the modeling framework used. By leveraging advanced machine learning techniques, the objective is to create a robust, accurate, and generalizable model that captures the relationships between key variables and the target outcome.

```{r}
manufacturing_tr <- read.csv("https://raw.githubusercontent.com/MarjeteV/data624/refs/heads/main/imputed_test_data.csv")

colnames(manufacturing_tr) <- gsub(" ", "_", colnames(manufacturing_tr))
brand_code_col <- c("Brand.CodeA","Brand.CodeB","Brand.CodeC","Brand.CodeD")

```

#### 5.1 Support Vector Machine

This section outlines the training and tuning of a Support Vector Machine with Gaussian Radial Basis Function Kernel to predict pH levels, leveraging the model's ability to handle non-linear relationships. The SVM model was selected due to its flexibility in capturing complex patterns in the data, making it well-suited for the task. Using a Gaussian Radial Basis Function (RBF) kernel, the SVM transforms input data into a higher-dimensional space by computing the similarity between data points. This transformation enables the model to find an optimal decision boundary or regression function in the new space, effectively capturing non-linear relationships between predictors and the target variable. This approach is particularly valuable for addressing the variability observed in pH levels, where intricate interactions among features may influence the outcome.

##### 5.1.1 Support Vector Machine Model Preprocessing

The dataset is split into training (75%) and testing (25%) subsets using random sampling to ensure the model is trained on a representative and diverse portion of the data while reserving an independent set for unbiased performance validation. Feature selection is applied prior to fitting a Radial Support Vector Machine (SVM) model to enhance model performance and efficiency. Recursive Feature Elimination (RFE) is employed as a robust method to identify the most relevant predictors by systematically ranking features based on their importance and iteratively removing those with minimal contribution. The dataset includes a categorical variable, Brand Code, which is transformed using target encoding. Target encoding replaces each category with the mean of the target variable for that category, allowing the relationship between the categorical variable and the target to be represented numerically. This transformation ensures compatibility with the RFE process by converting the categorical variable into a format that can be used effectively in feature selection. A Random Forest (rfFuncs) is used to evaluate feature importance, with performance assessed through 5-fold cross-validation to ensure the reliability and robustness of the selected feature subset.

```{r}
set.seed(8675309)

trainIndex <- sample(1:nrow(manufacturing_tr), size = 0.75 * nrow(manufacturing_tr))

beverage_man_train <- manufacturing_tr[trainIndex, ]
beverage_man_test <- manufacturing_tr[-trainIndex, ]

```

```{r}
rfe_dataset <- beverage_man_train %>%
  rowwise() %>%
  mutate(brand_code = case_when(
    Brand.CodeA == 1 ~ "A",
    Brand.CodeB == 1 ~ "B",
    Brand.CodeC == 1 ~ "C",
    Brand.CodeD == 1 ~ "D",
    TRUE ~ NA_character_
  )) %>%
  ungroup() %>% group_by(brand_code) %>% 
  mutate(brand_code_encoded = mean(PH)) |> ungroup() |>
  select(-c(Brand.CodeA, Brand.CodeB, Brand.CodeC, Brand.CodeD,"brand_code")) 
  
```

```{r message=FALSE, warning=FALSE}
set.seed(8675309)
ignore_col <- c("PH")

control <- rfeControl(functions = rfFuncs, method = "cv", number = 5,
                      allowParallel = T)

# Perform RFE
rfe_results <- rfe(
    rfe_dataset |> select(-all_of(ignore_col)),
  rfe_dataset$PH, 
  sizes = c(1:length(rfe_dataset)),
  rfeControl = control
)
```

```{r}
plot(rfe_results)
```

```{r}
rfeMaxR2 <- max(rfe_results$resample[,"RMSE"])
rfeMinR2 <- min(rfe_results$resample[,"RMSE"])

rfe_results$resample |> kable(caption = "Recursive Feature Elimination Results") |> kable_styling() |>  kable_classic()
```

The plot generated from the Recursive Feature Elimination (RFE) process shows that the model achieves low RMSE values with the `r rfe_results$bestSubset`-feature subset, indicating strong predictive performance during feature selection. Across the 5-fold cross-validation, the results are consistent, with RMSE, R², and MAE values remaining stable across iterations. The low RMSE and MAE confirm the model's accuracy and stability, while R² values, ranging from `r rfeMinR2` to `r rfeMaxR2`, suggest that the model has the potential to explain a reasonable portion of the variance in the target variable. These consistent metrics highlight the robustness of the model and validate the `r rfe_results$bestSubset`-feature subset as a reliable choice for prediction.

```{r}
varImportance <- varImp(rfe_results)
rfe_importance_df <- data.frame(Variable= rownames(varImportance),
                                Overall=varImportance$Overall ) 

rfe_importance_df |> ggplot( aes(y = reorder(Variable, +Overall), x = Overall)) + geom_bar(stat = "identity") + labs(
    title = "Variable Importance",
    x = "Overall Score",
    y = "Variable"
  )  + theme_minimal()
```

```{r}
imp_predictors <- predictors(rfe_results)
best_predictors <- append(brand_code_col,imp_predictors[imp_predictors != "brand_code_encoded"])

```

The Recursive Feature Elimination (RFE) results provide a clear ranking of predictors, emphasizing their contributions to the model’s performance. The top-ranked variable, brand_code_encoded, underscores the importance of capturing brand-specific patterns through target encoding, as it strongly correlates with the target variable. Among the operational process variables, Mnf.Flow, Oxygen.Filler, Pressure.Vacuum, and Temperature stand out, reflecting their critical role in driving variability in the target. These variables highlight the influence of flow rates, oxygen levels, pressure conditions, and environmental factors in the manufacturing process, which are essential for maintaining product quality.

Additional contributors, such as Usage.cont, Carb.Rel, and Balling.Lvl, showcase the importance of operational and compositional properties in fine-tuning predictions. While variables like Filler.Speed, Carb.Flow, and Balling rank lower, they still provide valuable supplementary information about the operational flow and composition dynamics.

Features such as Density, Bowl.Setpoint, and Filler.Level rank among the lowest, indicating limited direct impact or indirect influence through higher-ranked variables. Similarly, Carb.Pressure1 and Hyd.Pressure3 show minimal importance, suggesting their contribution to the target is captured by other operational metrics.

The RFE results highlight a robust combination of categorical, operational, and compositional variables, with brand_code_encoded and key operational factors leading the rankings. The brand_code_encoded variable, while ranked as the most significant, will be provided to the final model in its one-hot encoded format to ensure compatibility with the Support Vector Machine (SVM) regression model.

##### 5.1.2 Support Vector Machine Model Setup

This section details the implementation and tuning of a Support Vector Machine (SVM) regression model with a Gaussian Radial Basis Function (RBF) kernel to predict pH levels in the manufacturing process. The model is trained on a carefully selected set of predictors, which will be preprocessed using centering and scaling techniques. These preprocessing steps ensure that all variables contribute proportionally to the model and meet the requirements for SVM, which is sensitive to the magnitude of features.

To optimize model performance, hyperparameter tuning was conducted using a systematic grid search approach. This process explored a range of values for two critical parameters: the kernel width (sigma) and the regularization parameter (C). The kernel width (sigma) was varied from 0.01 to 0.2 in increments of 0.01, controlling the locality of the RBF kernel and determining how far its influence extends in the feature space. The regularization parameter (C) was tested over a range of 1 to 5 in increments of 1, balancing the trade-off between minimizing errors on the training data and maintaining a simpler, more generalizable model. Together, these parameters define the flexibility of the regression function and its ability to manage prediction deviations.

The training process incorporated cross-validation to evaluate model performance and ensure the selected hyperparameters generalize well to unseen data. This approach reduces the risk of overfitting and helps develop a predictive model that is both accurate and robust, meeting regulatory requirements for monitoring and reporting pH variability in the manufacturing process.

```{r message=FALSE, warning=FALSE}
set.seed(8675309)

svmRTuned <- train(
           x=beverage_man_train |>  select(all_of(best_predictors)),

                y=beverage_man_train$PH, 
                   method = "svmRadial",
                   preProcess = c("center", "scale"),
                    tuneGrid = expand.grid(sigma = seq(0.01,0.2,0.01), C = seq(1,5,1)),

                   trControl = trainControl(method = "cv",allowParallel = TRUE))


```

```{r}
svmRTuned$finalModel
```

```{r}
plot(svmRTuned)
```

```{r}
sigmaParam <- svmRTuned$bestTune[1,"sigma"]
cParam <- svmRTuned$bestTune[1,"C"]

svmResults <- svmRTuned$results |> filter(sigma == sigmaParam & C==cParam)
svmResults |> kable(caption = "SVM Training Set Evaluation Metrics") |> kable_styling() |>  kable_classic()

```

The results of the cross-validated SVM model with a sigma value of `r sigmaParam` and a cost parameter (C) of `r cParam` demonstrate robust performance. The model achieves a low RMSE of `r svmResults[,"RMSE"]` and an MAE of `r svmResults[,"MAE"]`, indicating accurate and consistent predictions on the scaled data. The R-squared value of `r svmResults[,"Rsquared"]` suggests the model explains approximately `r svmResults[,"MAE"]*100` of the variance in the target variable, showing moderate explanatory power. The standard deviations for RMSE (`r svmResults[,"RMSESD"]`), R-squared (`r svmResults[,"RsquaredSD"]`), and MAE (`r svmResults[,"MAESD"]`) across resampling folds are small, highlighting the stability of the model's performance across different data splits. These metrics indicate that the chosen parameters produce a reliable and well-generalized model for the given training dataset.

Building on these results, it is essential to evaluate the model's consistency across training and test datasets to ensure its robustness and generalizability. By comparing cross-validation metrics with test set performance, we can assess whether the SVM model maintains its predictive accuracy and explanatory power when applied to unseen data.

```{r}
svmPred <- predict(svmRTuned,newdata = beverage_man_test |> select(all_of(best_predictors)))

svm_test_post <- postResample(pred = svmPred, obs = beverage_man_test$PH) |> as.data.frame() 

svm_test_metrics <- data.frame(RMSE=svm_test_post[1,1],Rsquared=svm_test_post[2,1],MAE=svm_test_post[3,1])
```

```{r}
svm_test_metrics |> kable(caption = "SVM Test Set Evaluation Metrics") |> kable_styling() |>  kable_classic()
```

The SVM model demonstrates consistent performance between the training and test datasets, as indicated by the metrics from cross-validation and post-resample evaluation. The RMSE during training, (`r svmResults[,"RMSE"]`), is nearly identical to the test RMSE, (`r svmResults[,"RMSE"]`), suggesting stable predictive accuracy. Similarly, the MAE values are close, with (`r svmResults[,"MAE"]`) in training and (`r svmResults[,"MAE"]`) on the test set, reflecting consistent error magnitudes. The R-squared value shows a minor decrease from (`r svmResults[,"Rsquared"]`) in training to (`r svmResults[,"Rsquared"]`) on the test set, indicating that the model maintains reasonable explanatory power without significant overfitting. These results suggest that the model generalizes well and is reliable for making predictions on unseen data.

Overall, the SVM model demonstrates moderate predictive performance, with consistent metrics across training and test datasets that highlight its robustness and reliability. While the RMSE and MAE values indicate good predictive accuracy, the R-squared suggests room for improvement in explaining the variance of the target variable. These results establish a solid benchmark for comparison with other models.

```{r}
imputed_data <- read_csv("imputed_test_data.csv", show_col_types = FALSE)

```
```{r}
# Split
train_index <- createDataPartition(imputed_data$PH, p = 0.75, list = FALSE)
train_data <- imputed_data[train_index, ]
test_data <- imputed_data[-train_index, ]

```
#### PLS
Predictive modeling with partial least squares (PLS) is an effective technique, especially with datasets containing multiple correlated predictor variables. As PLS can identify key relationships between predictors and outcomes, it is ideal for understanding how PH levels in production are determined. PLS performs well by extracting the most relevant variability from both predictors and the outcome,  and  PLS can handle high-dimensional data. However, challenges are associated with the PLS approach, such as interpretability in understanding its latent components. PLS assumes linear relationships and may overemphasize high-variance predictors, limiting its effectiveness with nonlinear interactions or less relevant variables.
 
The PLS regression in this code uses cross-validation to minimize the Root Mean Squared Error of Prediction and determine the optimal number of components, evaluating up to 10 to balance flexibility and simplicity. The model achieved a Test RMSE of 0.141, an R² of 0.304, and a Test MAE of 0.111. While RMSE and MAE indicate reasonable predictions, the low R² value shows the model struggles to explain much of PH's variability, showing its limited predictive power.


```{r}
pls_model <- plsr(PH ~ ., data = train_data, ncomp = 10, validation = "CV")

optimal_components <- which.min(RMSEP(pls_model)$val[1, , -1])

pls_final_model <- plsr(PH ~ ., data = train_data, ncomp = optimal_components)

# Predictions and performance evaluation
predictions <- predict(pls_final_model, newdata = test_data, ncomp = optimal_components)
rmse <- sqrt(mean((test_data$PH - predictions)^2))
r_squared <- cor(test_data$PH, predictions)^2
mae <- mean(abs(test_data$PH - predictions))

cat("Optimal Components:", optimal_components, "\n")
cat("Test RMSE:", rmse, "\n")
cat("Test R²:", r_squared, "\n")
cat("Test MAE:", mae, "\n")
```
#### CART

Classification And Regression Trees (CART) is a non-parametric decision tree algorithm. For regression, CART divides the data based on the values of specific input predictors to produce a continuous target variable. CART is a valuable tool for predicting PH in manufacturing because it can handle non-linear relationships and interactions between predictors. Other advantages include its interpretability, the tree structure makes it easy to understand and communicate results. It is robust to outliers and requires minimal preprocessing, as it does not depend on scaling or transformation of predictors. However, CART has limitations, as it tends to overfit with overly complex trees that capture noise, though pruning can reduce this at the risk of oversimplifying the model.  CART can be unstable, where small data changes cause big tree adjustments and may underperform compared to advanced methods like ensembles. Still, it remains a valuable tool when properly tuned and validated.
 
The code uses the caret package to train an optimized CART model for predicting PH, leveraging parallel processing to improve efficiency. A tuning grid is defined for the complexity parameter (cp), ranging from 0.001 to 0.05, to identify the best pruning level. Five-fold cross-validation ensures the model is robust and avoids overfitting. The best cp value, 0.001, is selected based on cross-validation results, and the model's performance is evaluated on the test dataset using RMSE, R², and MAE. The results show a Test RMSE of 0.125, an R² of 0.483, and a Test MAE of 0.090, indicating the model explains only part of the variability in PH, highlighting some limitations despite effective tuning.

```{r}
#tuning grid
tune_grid <- expand.grid(
  cp = seq(0.001, 0.05, by = 0.005)  
)

optimized_train_control <- trainControl(
  method = "cv",
  number = 5,           
  verboseIter = FALSE,  
  allowParallel = TRUE
)

# Train  using caret
optimized_cart_model <- train(
  PH ~ ., 
  data = train_data,
  method = "rpart",
  trControl = optimized_train_control, 
  tuneGrid = tune_grid  
)

best_hyperparameters <- optimized_cart_model$bestTune
cat("Best Hyperparameter (cp):\n")
print(best_hyperparameters)

final_predictions <- predict(optimized_cart_model, newdata = test_data)

cart_rmse <- sqrt(mean((test_data$PH - final_predictions)^2))
cart_r_squared <- cor(test_data$PH, final_predictions)^2
cart_mae <- mean(abs(test_data$PH - final_predictions))

cat("Optimized CART Test RMSE:", cart_rmse, "\n")
cat("Optimized CART Test R²:", cart_r_squared, "\n")
cat("Optimized CART Test MAE:", cart_mae, "\n")

```

```{r read and split data, message=FALSE, echo=FALSE}
imputed_training_set <- read_csv("https://raw.githubusercontent.com/MarjeteV/data624/refs/heads/main/imputed_test_data.csv")

in_train <- createDataPartition(imputed_training_set$PH, p = 0.75, times = 1, list = FALSE)
caret_train <- imputed_training_set[ in_train,]
caret_test  <- imputed_training_set[-in_train,]
```

#### Cubist

$RMSE = 0.09115836$
$R^2 = 0.70848454$

Cubist models are rules based models that make use of an amalgamation of other techniques, including boosting, linear model smoothing, and adjustments based on nearby data points. The model was generated using the tuning parameters as shown below.

```{r cubist model, message=FALSE}
cubist_grid <- expand.grid(committees = c(1, 10, 50, 100), neighbors = seq(0,9))
model_control <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 5,
                           allowParallel = TRUE)

cubist_model <- train(
  PH ~ .,
  data = caret_train,
  method = "cubist",
  tuneGrid = cubist_grid,
  trControl = model_control
  )
```

Plotting the cubist model performance over our tuning parameters. We can see performance largely levels off beyond 5 instances, and there is no benefit beyond 50 committees. 

```{r plot cubist}
ggplot(cubist_model)
```

```{r performance cubist}
cubist_model_pred <- predict(cubist_model, newdata = caret_test)
postResample(pred = cubist_model_pred, obs = caret_test$PH)
```

The resulting model had an RMSE of 0.09115836 and an $R^2$ of 0.70848454. While one of the best models generated, we are deciding not to go with the Cubist as the XGBoost model had a similar level of performance. While the rules of the Cubist model give some insight into the decision making process of the model, there is no universally agreed upon method of determining predictor importance. Additionally, the use of committees and neighbor adjustments obfuscates the interpretability further. As the different methods of determining predictor importance can provide significantly different interpretations of the model, and the number of rules is extremely large, XGBoost stands as the more interpretable model. As they have very similar performance, interpretability is the deciding factor.

#### Ridge

$RMSE = 0.1293958$
$R^2 = 0.4102869$

Ridge regression models are an improvement upon PLS and deal with highly dimensional data as well as highly correlated data. They are also highly resilient to overfitting which can plague other models. As a linear model without feature selection it is highly interpretable, and generally provides a good baseline of performance. The  model was tuned using the code below.

```{r ridge model, message=FALSE}
ridgeGrid <- expand.grid(lambda = seq(0, 0.03, by = 0.005))

ridge_model <- train(
  PH ~ .,
  data = caret_train,
  method = "ridge",
  preProcess = c("center", "scale", "corr"),
  tuneGrid = ridgeGrid,
  trControl = model_control
  )
```

The resulting model performed best with a decay of 0.2 as seen in the plot below. The model had an RMSE of 0.1293958 and $R^2$ of 0.4102869 which, while respectable for a linear model, does not compete with the other models under consideration.

```{r plot ridge}
ggplot(ridge_model)
```

```{r performance ridge}
ridge_model_pred <- predict(ridge_model, newdata = caret_test)
postResample(pred = ridge_model_pred, obs = caret_test$PH)
```

#### LASSO

$RMSE = 0.1266806$  
$R^2 = 0.4346677$

LASSO regression is an extension of Ridge Regression which, in addition to regularizing, conducts feature selection. The model was generated using the code below.

```{r lasso model, message=FALSE}
lassoGrid <- expand.grid(.fraction = seq(.05, 1, length = 20))

lasso_model <- train(
  PH ~ .,
  data = caret_train,
  method = "lasso",
  preProcess = c("center", "scale"),
  tuneGrid = lassoGrid,
  trControl = model_control
  )
```

The resulting model had performance largely level off with a regression coefficient of 0.75. The resulting model had an RMSE of 0.1266806 and an $R^2$ of 0.4346677 which is practically equivalent to the Ridge model

```{r plot lasso}
ggplot(lasso_model)
```

```{r performance lasso}
lasso_model_pred <- predict(lasso_model, newdata = caret_test)
postResample(pred = lasso_model_pred, obs = caret_test$PH)
```

#### ENET

$RMSE = 0.1267312$  
$R^2 = 0.4342081$

Elastic Net models are a generalization of LASSO which makes use of multiple tuning parameters to get the benefits of the regularization of Ridge models with the feature selection of LASSO models. The model was tuned using the code below.

```{r Elastic Net model, message=FALSE}
enetGrid <- expand.grid(.lambda = c(0, 0.01, .1), .fraction = seq(.05, 1, length = 20))

enet_model <- train(
  PH ~ .,
  data = caret_train,
  method = "enet",
  preProcess = c("center", "scale"),
  tuneGrid = enetGrid,
  trControl = model_control
  )
```

The resulting model demonstrates that linear models are likely not the best solution to predicting this data, as model performance improved as more of the solution was added. The final model had an RMSE of 0.1267312 and 0.4342081 performing equivalently to the Ridge and LASSO models.

```{r plot enet}
ggplot(enet_model)
```

```{r performance enet}
enet_model_pred <- predict(enet_model, newdata = caret_test)
postResample(pred = enet_model_pred, obs = caret_test$PH)
```


### 6. Model Selection

### 7. Conclusion

### 8. Annotated References

1.  **Anton Paar Wiki. (n.d.).** *Carbon Dioxide in Beverages.*\
    [Carbon Dioxide in Beverages](https://wiki.anton-paar.com/au-en/carbon-dioxide-in-beverages/)\
    This resource provides an in-depth understanding of the role of carbon dioxide in beverages, including its effect on carbonation, fizziness, and product quality. It directly supports our analysis of variables like **Carb Flow** and **Carb Pressure**, which influence carbonation and pH levels.

2.  **Omega. (n.d.).** *What is pH?*\
    [What is pH?](https://www.omega.com/en-us/resources/what-is-ph)\
    This article explains the concept of pH, its measurement, and its relevance to various industries. It is useful for understanding the chemical principles behind pH variability in beverages and helps contextualize our target variable within the manufacturing process.

3.  **Emerson. (n.d.).** *Training Beverage Process Solutions Guide on De-Aeration.*\
    [Training Beverage Process Solutions Guide on De-Aeration](https://www.emerson.com/documents/automation/training-beverage-process-solutions-guide-on-de-aeration-micro-motion-en-63728.pdf)\
    This document focuses on de-aeration processes in beverage production, particularly the removal of dissolved oxygen and its impact on carbonation and quality. It directly relates to variables like **Oxygen.Filler** and **Pressure.Vacuum**, providing insights into their operational importance.

4.  **Jochamp. (n.d.).** *Carbonated Beverages Manufacturing Process - A Step by Step Guide.*\
    [Carbonated Beverages Manufacturing Process - A Step by Step Guide](https://jochamp.com/carbonated-beverages-manufacturing-process/)\
    This guide offers a comprehensive overview of the carbonation process in beverage production, including the role of temperature, pressure, and filling speed. It supports our understanding of variables like **Temperature**, **Filler.Speed**, and **Carb Pressure**, helping us interpret their influence on product quality.
